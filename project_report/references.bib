@inproceedings{plepi-etal-2022-unifying,
    title = "Unifying Data Perspectivism and Personalization: An Application to Social Norms",
    author = "Plepi, Joan  and
      Neuendorf, B{\'e}la  and
      Flek, Lucie  and
      Welch, Charles",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.500",
    doi = "10.18653/v1/2022.emnlp-main.500",
    pages = "7391--7402",
    abstract = "Instead of using a single ground truth for language processing tasks, several recent studies have examined how to represent and predict the labels of the set of annotators. However, often little or no information about annotators is known, or the set of annotators is small. In this work, we examine a corpus of social media posts about conflict from a set of 13k annotators and 210k judgements of social norms. We provide a novel experimental setup that applies personalization methods to the modeling of annotators and compare their effectiveness for predicting the perception of social norms. We further provide an analysis of performance across subsets of social situations that vary by the closeness of the relationship between parties in conflict, and assess where personalization helps the most.",
}

@inproceedings{forbes-etal-2020-social,
    title = "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
    author = "Forbes, Maxwell  and
      Hwang, Jena D.  and
      Shwartz, Vered  and
      Sap, Maarten  and
      Choi, Yejin",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.48",
    doi = "10.18653/v1/2020.emnlp-main.48",
    pages = "653--670",
    abstract = "Social norms{---}the unspoken commonsense rules about acceptable social behavior{---}are crucial in understanding the underlying causes and intents of people{'}s actions in narratives. For example, underlying an action such as {``}wanting to call cops on my neighbor{''} are social norms that inform our conduct, such as {``}It is expected that you report crimes.{''} We present SOCIAL CHEMISTRY, a new conceptual formalism to study people{'}s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce SOCIAL-CHEM-101, a large-scale corpus that catalogs 292k rules-of-thumb such as {``}It is rude to run a blender at 5am{''} as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people{'}s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions. Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes SOCIAL-CHEM-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.",
}
@inproceedings{binsted1994implemented,
  title        = {An Implemented Model of Punning Riddles},
  author       = {Binsted, Kim and Ritchie, Graeme},
  booktitle    = {Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94)},
  year         = {1994},
  pages        = {1--6},
  publisher    = {AAAI Press}
}
@article{stock2003hahacronym,
  title        = {HAHAcronym: Humorous Agents for Humorous Acronyms},
  author       = {Stock, Oliviero and Strapparava, Carlo},
  journal      = {Humor: International Journal of Humor Research},
  volume       = {16},
  number       = {3},
  pages        = {297--314},
  year         = {2003}
}
@article{su2025survey,
  title={A Survey of Pun Generation: Datasets, Evaluations and Methodologies},
  author={Su, Yuchen and Zhu, Yonghua and Wang, Ruofan and Huang, Zijian and Benavides-Prado, Diana and Witbrock, Michael},
  journal={Findings of the Association for Computational Linguistics: EMNLP 2025},
  pages={7375--7395},
  year={2025}
}

@article{comic_toolbox,
  title={The Comic Toolbox: How to Be Funny Even If You're Not},
  author={John Vorhaus },
  publisher={Silman-James Press,U.S.},
  year={1994}
}


@inproceedings{mittal2022ambipun,
  title        = {AmbiPun: Generating Humorous Puns with Ambiguous Context},
  author       = {Mittal, Anirudh and Tian, Yufei and Peng, Nanyun},
  booktitle    = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year         = {2022},
  pages        = {1053--1062},
  publisher    = {Association for Computational Linguistics},
  doi          = {10.18653/v1/2022.naacl-main.77}
}

@article{toplyn2023witscript,
  title={Witscript 3: A hybrid ai system for improvising jokes in a conversation},
  author={Toplyn, Joe},
  journal={arXiv preprint arXiv:2301.02695},
  year={2023}
}

@article{vikhorev2024cleancomedy,
  title={CleanComedy: Creating Friendly Humor through Generative Techniques},
  author={Vikhorev, Dmitry and Galimzianova, Daria and Gorovaia, Svetlana and Zhemchuzhina, Elizaveta and Yamshchikov, Ivan P},
  journal={arXiv preprint arXiv:2412.09203},
  year={2024}
}

@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@article{dettmers2023qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={10088--10115},
  year={2023}
}

@inproceedings{NEURIPS2020_6b493230,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{carbonell,
author = {Carbonell, Jaime and Goldstein, Jade},
title = {The use of MMR, diversity-based reranking for reordering documents and producing summaries},
year = {1998},
isbn = {1581130155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/290941.291025},
doi = {10.1145/290941.291025},
booktitle = {Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {335â€“336},
numpages = {2},
location = {Melbourne, Australia},
series = {SIGIR '98}
}
@misc{fu2023gptscoreevaluatedesire,
      title={GPTScore: Evaluate as You Desire}, 
      author={Jinlan Fu and See-Kiong Ng and Zhengbao Jiang and Pengfei Liu},
      year={2023},
      eprint={2302.04166},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.04166}, 
}

@misc{liu2023gevalnlgevaluationusing,
      title={G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment}, 
      author={Yang Liu and Dan Iter and Yichong Xu and Shuohang Wang and Ruochen Xu and Chenguang Zhu},
      year={2023},
      eprint={2303.16634},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.16634}, 
}

@misc{sakabe2025assessingcapabilitiesllmshumora,
      title={Assessing the Capabilities of LLMs in Humor:A Multi-dimensional Analysis of Oogiri Generation and Evaluation}, 
      author={Ritsu Sakabe and Hwichan Kim and Tosho Hirasawa and Mamoru Komachi},
      year={2025},
      eprint={2511.09133},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2511.09133}, 
}

@inproceedings{meaney-etal-2021-semeval,
    title = "{S}em{E}val 2021 Task 7: {H}a{H}ackathon, Detecting and Rating Humor and Offense",
    author = "Meaney, J. A.  and
      Wilson, Steven  and
      Chiruzzo, Luis  and
      Lopez, Adam  and
      Magdy, Walid",
    editor = "Palmer, Alexis  and
      Schneider, Nathan  and
      Schluter, Natalie  and
      Emerson, Guy  and
      Herbelot, Aurelie  and
      Zhu, Xiaodan",
    booktitle = "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.semeval-1.9/",
    doi = "10.18653/v1/2021.semeval-1.9",
    pages = "105--119",
    abstract = "SemEval 2021 Task 7, HaHackathon, was the first shared task to combine the previously separate domains of humor detection and offense detection. We collected 10,000 texts from Twitter and the Kaggle Short Jokes dataset, and had each annotated for humor and offense by 20 annotators aged 18-70. Our subtasks were binary humor detection, prediction of humor and offense ratings, and a novel controversy task: to predict if the variance in the humor ratings was higher than a specific threshold. The subtasks attracted 36-58 submissions, with most of the participants choosing to use pre-trained language models. Many of the highest performing teams also implemented additional optimization techniques, including task-adaptive training and adversarial training. The results suggest that the participating systems are well suited to humor detection, but that humor controversy is a more challenging task. We discuss which models excel in this task, which auxiliary techniques boost their performance, and analyze the errors which were not captured by the best systems."
}