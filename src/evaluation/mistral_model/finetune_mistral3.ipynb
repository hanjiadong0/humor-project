{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d1c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# === 0) Install (wie im Notebook) ===\n",
    "# %pip install -q accelerate peft bitsandbytes transformers datasets evaluate scikit-learn\n",
    "# %pip install transformers==5.0.0rc0\n",
    "# %pip install mistral-common --upgrade\n",
    "import os, torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model,PeftModel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, mean_absolute_error, root_mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f0a61",
   "metadata": {},
   "source": [
    "Model + dataset IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"mistralai/Ministral-3-8B-Base-2512\"\n",
    "\n",
    "# Your dataset must contain:\n",
    "# - \"text\" (string)\n",
    "# - \"label_cls\" (int class id, e.g. 0/1 or 0..K-1)\n",
    "# - \"label_reg\" (float, e.g. funniness score)\n",
    "dataset_path = \"YOUR_DATASET_PATH_OR_HF_DATASET_NAME\"\n",
    "\n",
    "num_labels = 2          # <-- set your number of classes\n",
    "loss_w_reg = 1.0        # <-- weight for regression loss\n",
    "max_length = 512        # keep smaller first (faster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5de7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'max_position_embeddings'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350ac76e86c4431e8f935e239ed5c95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mistral3Model LOAD REPORT from: mistralai/Ministral-3-8B-Base-2512\n",
      "Key                           | Status     |  | \n",
      "------------------------------+------------+--+-\n",
      "language_model.lm_head.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "backbone = AutoModel.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "backbone = prepare_model_for_kbit_training(backbone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eac812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'max_position_embeddings'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba63591f11343a7bff9bfd6e067441f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mistral3Model LOAD REPORT from: mistralai/Ministral-3-8B-Base-2512\n",
      "Key                           | Status     |  | \n",
      "------------------------------+------------+--+-\n",
      "language_model.lm_head.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "# === 3) Load model backbone in 4-bit (QLoRA wie im Notebook) ===\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "backbone = AutoModel.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "backbone = prepare_model_for_kbit_training(backbone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b51b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_config(cfg):\n",
    "    # Mistral3Config: wrapper => text_config contains the LM config\n",
    "    if hasattr(cfg, \"text_config\") and cfg.text_config is not None:\n",
    "        return cfg.text_config\n",
    "    return cfg\n",
    "\n",
    "def get_hidden_size(cfg) -> int:\n",
    "    tc = get_text_config(cfg)\n",
    "\n",
    "    # tc can be a config object OR a dict\n",
    "    if isinstance(tc, dict):\n",
    "        for k in [\"hidden_size\", \"dim\", \"d_model\", \"model_dim\"]:\n",
    "            if k in tc and tc[k] is not None:\n",
    "                return int(tc[k])\n",
    "        raise ValueError(f\"hidden size not found in text_config dict keys={list(tc.keys())}\")\n",
    "\n",
    "    # config object\n",
    "    for attr in [\"hidden_size\", \"dim\", \"d_model\", \"model_dim\"]:\n",
    "        if hasattr(tc, attr) and getattr(tc, attr) is not None:\n",
    "            return int(getattr(tc, attr))\n",
    "\n",
    "    # last fallback\n",
    "    if hasattr(tc, \"to_dict\"):\n",
    "        d = tc.to_dict()\n",
    "        for k in [\"hidden_size\", \"dim\", \"d_model\", \"model_dim\"]:\n",
    "            if k in d and d[k] is not None:\n",
    "                return int(d[k])\n",
    "\n",
    "    raise ValueError(\"Could not find hidden size in config / text_config.\")\n",
    "\n",
    "def get_text_backbone(model):\n",
    "    \"\"\"\n",
    "    Only unwrap if this is a multimodal wrapper (has config.text_config).\n",
    "    Prefer explicit text submodules when present.\n",
    "    Otherwise return the model itself (safe for AutoModel / CausalLM etc).\n",
    "    \"\"\"\n",
    "    cfg = getattr(model, \"config\", None)\n",
    "    is_wrapper = (cfg is not None) and hasattr(cfg, \"text_config\") and (cfg.text_config is not None)\n",
    "\n",
    "    if not is_wrapper:\n",
    "        return model\n",
    "\n",
    "    # multimodal wrapper: try known text module names\n",
    "    for name in [\"text_model\", \"language_model\"]:\n",
    "        if hasattr(model, name):\n",
    "            m = getattr(model, name)\n",
    "            if m is not None:\n",
    "                return m\n",
    "\n",
    "    # if no explicit text submodule, fall back to model itself\n",
    "    return model\n",
    "\n",
    "def last_token_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Take hidden state of the last *real* token (not padding).\n",
    "    last_hidden_state: [B, T, H]\n",
    "    attention_mask:   [B, T] with 1 for tokens, 0 for padding\n",
    "    \"\"\"\n",
    "    if attention_mask is None:\n",
    "        return last_hidden_state[:, -1, :]\n",
    "\n",
    "    lengths = attention_mask.long().sum(dim=1)  # [B]\n",
    "    idx = torch.clamp(lengths - 1, min=0)       # [B]\n",
    "    batch_idx = torch.arange(last_hidden_state.size(0), device=last_hidden_state.device)\n",
    "    return last_hidden_state[batch_idx, idx, :]  # [B, H]\n",
    "\n",
    "\n",
    "class MultiTaskRegCls(nn.Module):\n",
    "    def __init__(self, backbone, num_labels: int, loss_w_reg: float = 1.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.config = backbone.config   \n",
    "        h = get_hidden_size(backbone.config)   # <-- FIX: works for Mistral3Config wrapper\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cls_head = nn.Linear(h, num_labels)\n",
    "        self.reg_head = nn.Linear(h, 1)\n",
    "        self.loss_w_reg = loss_w_reg\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, y_cls=None, y_reg=None, **kwargs):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "\n",
    "        pooled = last_token_pool(out.last_hidden_state, attention_mask)\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        logits_cls = self.cls_head(pooled)               # [B,C]\n",
    "        pred_reg   = self.reg_head(pooled).squeeze(-1)   # [B]\n",
    "\n",
    "        # pack outputs for metrics\n",
    "        logits = torch.cat([logits_cls, pred_reg.unsqueeze(-1)], dim=1)  # [B, C+1]\n",
    "\n",
    "        loss = None\n",
    "        if (y_cls is not None) or (y_reg is not None):\n",
    "            loss = 0.0\n",
    "            if y_cls is not None:\n",
    "                loss = loss + F.cross_entropy(logits_cls, y_cls.long())\n",
    "            if y_reg is not None:\n",
    "                loss = loss + self.loss_w_reg * F.mse_loss(pred_reg.float(), y_reg.float())\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# backbone = AutoModel.from_pretrained(...)\n",
    "text_backbone = get_text_backbone(backbone)\n",
    "\n",
    "model = MultiTaskRegCls(\n",
    "    text_backbone,\n",
    "    num_labels=num_labels,\n",
    "    loss_w_reg= 1 ,   # or your variable name\n",
    "    dropout=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe0264",
   "metadata": {},
   "outputs": [],
   "source": "lora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"SEQ_CLS\",\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n    modules_to_save=[\"cls_head\", \"reg_head\"],  # FIX: save head weights (was missing, PEFT auto-added wrong names)\n)\n\nmodel = MultiTaskRegCls(backbone, num_labels=num_labels, loss_w_reg=loss_w_reg, dropout=0.1)\nmodel = get_peft_model(model, lora_config)\n\ntrainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | {trainable/total*100:.4f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaffe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "@dataclass\n",
    "class TrainingDataConfig:\n",
    "    csv_path: str\n",
    "    text_col: str = \"text\"\n",
    "    cls_col_raw: str = \"is_humor\"\n",
    "    reg_col_raw: str = \"humor_rating\"\n",
    "    missing_cls_value: int = -1\n",
    "    missing_reg_value: float = -1.0\n",
    "    val_size: float = 0.1\n",
    "\n",
    "class DataModule:\n",
    "    def __init__(self, cfg: Dict[str, Any], tcfg: TrainingDataConfig):\n",
    "        self.cfg = cfg\n",
    "        self.tcfg = tcfg\n",
    "\n",
    "    def load_dataframe(self) -> pd.DataFrame:\n",
    "        df = pd.read_csv(self.tcfg.csv_path)\n",
    "\n",
    "        # check columns exist\n",
    "        needed = [self.tcfg.text_col, self.tcfg.cls_col_raw, self.tcfg.reg_col_raw]\n",
    "        missing = [c for c in needed if c not in df.columns]\n",
    "        if missing:\n",
    "            raise KeyError(f\"Missing columns in CSV: {missing}. Found: {df.columns.tolist()[:50]}\")\n",
    "\n",
    "        # rename to internal standard names\n",
    "        df = df.rename(columns={\n",
    "            self.tcfg.text_col: \"text\",\n",
    "            self.tcfg.cls_col_raw: \"y_cls\",\n",
    "            self.tcfg.reg_col_raw: \"y_reg\",\n",
    "        })\n",
    "\n",
    "        # drop empty text\n",
    "        df[\"text\"] = df[\"text\"].astype(str)\n",
    "        df = df[df[\"text\"].str.strip().ne(\"\")].reset_index(drop=True)\n",
    "\n",
    "        # cls label (may have NaN)\n",
    "        df[\"y_cls\"] = pd.to_numeric(df[\"y_cls\"], errors=\"coerce\").fillna(self.tcfg.missing_cls_value).astype(\"int64\")\n",
    "\n",
    "        # reg label (may have NaN / strings)\n",
    "        df[\"y_reg\"] = pd.to_numeric(df[\"y_reg\"], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "        # enforce your consistency rule:\n",
    "        # - cls=0 => reg=-1\n",
    "        df.loc[df[\"y_cls\"] == 0, \"y_reg\"] = self.tcfg.missing_reg_value\n",
    "        # - cls=1 but missing reg => -1\n",
    "        df.loc[(df[\"y_cls\"] == 1) & (df[\"y_reg\"].isna()), \"y_reg\"] = self.tcfg.missing_reg_value\n",
    "\n",
    "        # clip only valid humor ratings\n",
    "        humor = df[\"y_reg\"] >= 0\n",
    "        df.loc[humor, \"y_reg\"] = df.loc[humor, \"y_reg\"].clip(0.0, 4.0)\n",
    "\n",
    "        # final strict rule: cls derived from reg sign\n",
    "        df[\"y_cls\"] = (df[\"y_reg\"] >= 0).astype(\"int64\")\n",
    "\n",
    "        return df[[\"text\", \"y_cls\", \"y_reg\"]]\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_fn(batch, tokenizer, max_length: int):\n",
    "        # IMPORTANT: no padding here (faster). Collator will pad dynamically.\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "    def build_datasets(self, tokenizer) -> Tuple[Dataset, Dataset]:\n",
    "        seed = int(self.cfg[\"project\"][\"seed\"])\n",
    "        max_len = int(self.cfg[\"model\"][\"max_length\"])\n",
    "\n",
    "        df = self.load_dataframe()\n",
    "        train_df, val_df = train_test_split(df, test_size=self.tcfg.val_size, random_state=seed, shuffle=True)\n",
    "\n",
    "        train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "        val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "        train_ds = train_ds.map(lambda b: self.tokenize_fn(b, tokenizer, max_len), batched=True, remove_columns=[\"text\"])\n",
    "        val_ds   = val_ds.map(lambda b: self.tokenize_fn(b, tokenizer, max_len), batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "        # keep labels + inputs\n",
    "        cols = [\"input_ids\", \"attention_mask\", \"y_cls\", \"y_reg\"]\n",
    "        train_ds = train_ds.select_columns(cols)\n",
    "        val_ds   = val_ds.select_columns(cols)\n",
    "\n",
    "        return train_ds, val_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d35ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  y_cls  y_reg\n",
      "0  TENNESSEE: We're the best state. Nobody even c...      1   2.42\n",
      "1  A man inserted an advertisement in the classif...      1   2.50\n",
      "columns: ['text', 'y_cls', 'y_reg']\n",
      "y_cls unique sample: [1 0]\n",
      "y_reg min/max: -1.0 4.0\n",
      "(cls=0 but reg>=0): 0\n",
      "(cls=1 but reg<0): 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c149ee4ae97a44abadc91dae16755b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d86e3e0d60453993cc8e14c97c8d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rows: 7200 val rows: 800\n",
      "keys: dict_keys(['input_ids', 'attention_mask', 'y_cls', 'y_reg'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_dataset():\n",
    "    cfg: Dict[str, Any] = {\n",
    "        \"project\": {\"seed\": 42},\n",
    "        \"model\": {\"max_length\": 64},\n",
    "    }\n",
    "\n",
    "    tcfg = TrainingDataConfig(\n",
    "        csv_path=r\"C:\\Users\\Anwender\\humor-project\\data\\labels\\hahackathon_train.csv\",\n",
    "        text_col=\"text\",\n",
    "        cls_col_raw=\"is_humor\",\n",
    "        reg_col_raw=\"humor_rating\",\n",
    "        missing_cls_value=-1,\n",
    "        missing_reg_value=-1.0,\n",
    "        val_size=0.1,\n",
    "    )\n",
    "\n",
    "    dm = DataModule(cfg, tcfg)\n",
    "\n",
    "    # 1) dataframe checks\n",
    "    df = dm.load_dataframe()\n",
    "    print(df.head(2))\n",
    "    print(\"columns:\", df.columns.tolist())\n",
    "    print(\"y_cls unique sample:\", df[\"y_cls\"].unique()[:10])\n",
    "    print(\"y_reg min/max:\", df[\"y_reg\"].min(), df[\"y_reg\"].max())\n",
    "\n",
    "    assert {\"text\", \"y_cls\", \"y_reg\"} <= set(df.columns)\n",
    "\n",
    "    # property checks\n",
    "    bad1 = ((df[\"y_cls\"] == 0) & (df[\"y_reg\"] >= 0)).sum()\n",
    "    bad2 = ((df[\"y_cls\"] == 1) & (df[\"y_reg\"] < 0)).sum()\n",
    "    print(\"(cls=0 but reg>=0):\", bad1)\n",
    "    print(\"(cls=1 but reg<0):\", bad2)\n",
    "    assert bad1 == 0\n",
    "    assert bad2 == 0\n",
    "\n",
    "    # 2) datasets\n",
    "    tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    train_ds, val_ds = dm.build_datasets(tok)\n",
    "\n",
    "    print(\"train rows:\", len(train_ds), \"val rows:\", len(val_ds))\n",
    "    sample = train_ds[0]\n",
    "    print(\"keys:\", sample.keys())\n",
    "\n",
    "    assert set(sample.keys()) == {\"input_ids\", \"attention_mask\", \"y_cls\", \"y_reg\"}\n",
    "\n",
    "    # NOTE: because we removed padding=\"max_length\", lengths may vary now!\n",
    "    # so don't assert fixed length here.\n",
    "    assert len(sample[\"input_ids\"]) >= 1\n",
    "    assert len(sample[\"attention_mask\"]) == len(sample[\"input_ids\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa260432",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # ---- preds: could be array, tuple, list ----\n",
    "    if isinstance(preds, (tuple, list)):\n",
    "        preds = preds[0]\n",
    "    preds = np.asarray(preds)\n",
    "\n",
    "    # Expect packed logits: [N, C+1]\n",
    "    if preds.ndim != 2 or preds.shape[1] < 2:\n",
    "        raise ValueError(f\"Unexpected preds shape: {preds.shape}. Expected [N, C+1].\")\n",
    "\n",
    "    cls_logits = preds[:, :num_labels]   # [N,C]\n",
    "    reg_pred   = preds[:, -1]            # [N]\n",
    "\n",
    "    # ---- labels: could be tuple/list (y_cls, y_reg) or array ----\n",
    "    if isinstance(labels, (tuple, list)) and len(labels) == 2:\n",
    "        y_cls, y_reg = labels\n",
    "    else:\n",
    "        labels = np.asarray(labels)\n",
    "        # If Trainer stacked them as [N,2]\n",
    "        if labels.ndim == 2 and labels.shape[1] == 2:\n",
    "            y_cls, y_reg = labels[:, 0], labels[:, 1]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected labels shape/type: {type(labels)} {getattr(labels,'shape',None)}\")\n",
    "\n",
    "    y_cls = np.asarray(y_cls).astype(int)\n",
    "    y_reg = np.asarray(y_reg).astype(float)\n",
    "\n",
    "    cls_pred = cls_logits.argmax(axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_cls, cls_pred)\n",
    "    f1  = f1_score(y_cls, cls_pred, average=\"macro\")\n",
    "\n",
    "    mae  = mean_absolute_error(y_reg, reg_pred)\n",
    "    rmse = root_mean_squared_error(y_reg, reg_pred)\n",
    "    r2   = r2_score(y_reg, reg_pred)\n",
    "\n",
    "    return {\"cls_acc\": acc, \"cls_f1\": f1, \"reg_mae\": mae, \"reg_rmse\": rmse, \"reg_r2\": r2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a737d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints_ministral3_multitask\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps= 136,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    dataloader_num_workers=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c05369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af7dc792a724b3cb3e665722081c487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5492846282ba4373ac16d0c422795b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "cfg: Dict[str, Any] = {\n",
    "    \"project\": {\"seed\": 42},\n",
    "    \"model\": {\"max_length\": 64},\n",
    "}\n",
    "\n",
    "tcfg = TrainingDataConfig(\n",
    "    csv_path=r\"C:\\Users\\Anwender\\humor-project\\data\\labels\\hahackathon_train.csv\",\n",
    "    text_col=\"text\",\n",
    "    cls_col_raw=\"is_humor\",\n",
    "    reg_col_raw=\"humor_rating\",\n",
    "    missing_cls_value=-1,\n",
    "    missing_reg_value=-1.0,\n",
    "    val_size=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "dm = DataModule(cfg, tcfg)\n",
    "\n",
    "train_ds, val_ds = dm.build_datasets(tokenizer)\n",
    "\n",
    "# 1) cache off for training\n",
    "model.config.use_cache =  False  \n",
    "\n",
    "# 2) gradient checkpointing (optional)\n",
    "if hasattr(text_backbone, \"gradient_checkpointing_enable\"):\n",
    "    try:\n",
    "        text_backbone.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    except TypeError:\n",
    "        text_backbone.gradient_checkpointing_enable()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.label_names = [\"y_cls\", \"y_reg\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc997a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'max_position_embeddings'}\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131' max='5400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 131/5400 01:14 < 50:54, 1.72 it/s, Epoch 0.07/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}