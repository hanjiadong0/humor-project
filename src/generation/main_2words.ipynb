{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944232a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"main_v3_dual_evaluator.py\n",
    "\n",
    "LangGraph Joke Generation Pipeline with Dual Evaluation (GPT-OSS + Mistral)\n",
    "\"\"\"\n",
    "\n",
    "# !pip uninstall -y transformers accelerate peft bitsandbytes trl\n",
    "# !pip install -q -U bitsandbytes accelerate peft transformers trl datasets\n",
    "# !pip install faiss-cpu\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "import wandb\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad46c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Setting up environment...\n",
      "âœ“ GPU detected: NVIDIA GeForce RTX 5090 Laptop GPU\n",
      "  Memory: 25.65 GB\n",
      " Environment setup complete\n",
      "\n",
      "Using PROJECT_ROOT: c:\\Users\\Anwender\\humor-project\n",
      " All modules imported successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup environment and imports\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ”§ Setting up environment...\")\n",
    "\n",
    "# Check for GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected. Will use CPU (slower)\")\n",
    "\n",
    "print(\" Environment setup complete\\n\")\n",
    "\n",
    "\"\"\"\n",
    "LangGraph Joke Generation Pipeline - Orchestrator with Dual Evaluation\n",
    "Imports and orchestrates existing implementations\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "LangGraph Joke Generation Pipeline - Orchestrator\n",
    "Imports and orchestrates existing implementations\n",
    "\"\"\"\n",
    "from typing import TypedDict, Literal, List, Dict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "\n",
    "#  set project root = folder c:\\Users\\Anwender\\humor-project\n",
    "PROJECT_ROOT = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "\n",
    "# if you run from a notebook inside some subfolder, walk up until we find \"src\"\n",
    "while not (PROJECT_ROOT / \"src\").exists() and PROJECT_ROOT.parent != PROJECT_ROOT:\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))         # so \"import src....\" works\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\")) # so \"import evaluation....\" works if you want\n",
    "print(\"Using PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "# Import your existing implementations\n",
    "from using_llama_with_rag import RAGHumorGenerator, RAGConfig\n",
    "from gpt_joke_generation import GPT4JokeGenerator\n",
    "from src.evaluation.llm_evaluator import LLMHumorScorer\n",
    "from src.evaluation.mistral_evaluator import MistralHumorScorer\n",
    "\n",
    "print(\" All modules imported successfully\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f5b68",
   "metadata": {},
   "outputs": [],
   "source": "\n# ============================================================================\n# LANGGRAPH STATE DEFINITION\n# ============================================================================\n\nclass JokeGenerationState(TypedDict):\n    \"\"\"State for the joke generation pipeline\"\"\"\n    # Input\n    word1: str\n    word2: str\n\n    # Generation tracking\n    llama_jokes: List[str]\n    gpt_jokes: List[str]\n    candidate_jokes: List[str]\n    joke_sources: List[str]      # \"Llama\" or \"GPT\" per joke\n\n    # Scoring tracking\n    scores: List[float]\n    gpt_scores: List[float]      # Track GPT scores separately\n    mistral_scores: List[float]  # Track Mistral scores separately\n\n    # Best result tracking\n    best_joke: str\n    best_score: float\n\n    # Iteration tracking\n    iteration_count: int\n    max_iterations: int\n\n    # History for debugging\n    history: List[Dict]\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c7aa4",
   "metadata": {},
   "outputs": [],
   "source": "\n# ============================================================================\n# NODE FUNCTIONS\n# ============================================================================\n\ndef generate_jokes_llama(state: JokeGenerationState, llama_gen: RAGHumorGenerator) -> Dict:\n    \"\"\"Generate jokes using Llama model\"\"\"\n    word1 = state[\"word1\"]\n    word2 = state[\"word2\"]\n    iteration = state.get(\"iteration_count\", 0)\n\n    mode = \"headline\" if not word2 else \"word-pair\"\n    print(f\"\\n[Iteration {iteration + 1}] Generating jokes with Llama ({mode} mode)...\")\n\n    result_with_RAG = llama_gen.generate_with_rag(\n        word1=word1,\n        word2=word2,\n        use_rag=True,  # with RAG\n        num_candidates=3\n    )\n\n    result_without_RAG = llama_gen.generate_with_rag(\n        word1=word1,\n        word2=word2,\n        use_rag=False,  # No RAG - just fine-tuned model\n        num_candidates=3\n    )\n\n    jokes = result_with_RAG[\"generated_jokes\"] + result_without_RAG[\"generated_jokes\"]\n\n    print('result_with_RAG jokes:')\n    print(result_with_RAG[\"generated_jokes\"])\n\n    print('result_without_RAG jokes:')\n    print(result_without_RAG[\"generated_jokes\"])\n\n    print(f\"   Generated {len(jokes)} Llama jokes\")\n\n    return {\"llama_jokes\": jokes}\n\n\ndef generate_jokes_gpt(state: JokeGenerationState, gpt_gen: GPT4JokeGenerator) -> Dict:\n    \"\"\"Generate jokes using GPT-OSS (reduced to 2 to limit self-bias)\"\"\"\n    word1 = state[\"word1\"]\n    word2 = state[\"word2\"]\n    iteration = state.get(\"iteration_count\", 0)\n\n    mode = \"headline\" if not word2 else \"word-pair\"\n    print(f\"\\n[Iteration {iteration + 1}] Generating jokes with GPT-OSS ({mode} mode)...\")\n\n    jokes = gpt_gen.generate_jokes(word1, word2, num_jokes=2, verbose=False)\n\n    print(f\"   Generated {len(jokes)} GPT-OSS jokes\")\n\n    return {\"gpt_jokes\": jokes}\n\n\ndef combine_jokes(state: JokeGenerationState) -> JokeGenerationState:\n    \"\"\"Combine jokes from both generators and track sources\"\"\"\n    llama_jokes = state.get(\"llama_jokes\", [])\n    gpt_jokes = state.get(\"gpt_jokes\", [])\n\n    all_jokes = llama_jokes + gpt_jokes\n    sources = [\"Llama\"] * len(llama_jokes) + [\"GPT\"] * len(gpt_jokes)\n\n    print(f\"\\nCombined {len(all_jokes)} total jokes ({len(llama_jokes)} Llama + {len(gpt_jokes)} GPT)\")\n\n    return {\n        **state,\n        \"candidate_jokes\": all_jokes,\n        \"joke_sources\": sources,\n    }\n\n\ndef normalize_mistral_score(raw: float) -> float:\n    \"\"\"\n    Normalize Mistral humor score (0-4) to 0-10 scale using\n    percentile-aware mapping based on training data distribution.\n    \n    Training data stats (hahackathon_train.csv):\n      - Range: 0.1 - 4.0, Mean: 2.26, SD: 0.57\n      - ~90% of scores fall between 1.5 and 3.0\n      - Score 3.0+ = top quartile (genuinely funny)\n      - Score 1.5-  = bottom quartile (not funny)\n    \n    We use a piecewise linear mapping that stretches the meaningful\n    range (1.0-3.5) across most of the 0-10 scale:\n      Mistral 0.0 -> 0.0    (no humor)\n      Mistral 1.0 -> 2.0    (weak humor)\n      Mistral 2.0 -> 4.5    (below average)\n      Mistral 2.26-> 5.0    (average - matches training mean)\n      Mistral 3.0 -> 7.0    (good humor)\n      Mistral 3.5 -> 8.5    (very good)\n      Mistral 4.0 -> 10.0   (exceptional)\n    \"\"\"\n    raw = max(0.0, min(4.0, raw))\n    \n    # Piecewise linear mapping\n    breakpoints = [(0.0, 0.0), (1.0, 2.0), (2.0, 4.5), (2.26, 5.0),\n                   (3.0, 7.0), (3.5, 8.5), (4.0, 10.0)]\n    \n    for i in range(len(breakpoints) - 1):\n        x0, y0 = breakpoints[i]\n        x1, y1 = breakpoints[i + 1]\n        if raw <= x1:\n            t = (raw - x0) / (x1 - x0)\n            return y0 + t * (y1 - y0)\n    \n    return 10.0\n\n\ndef score_jokes_dual(\n    state: JokeGenerationState,\n    llm_scorer: LLMHumorScorer,\n    mistral_scorer: MistralHumorScorer\n) -> JokeGenerationState:\n    \"\"\"\n    Score jokes using dual evaluator ensemble.\n    \n    Scoring weights:\n    - GPT-OSS (60%): Multi-criteria quality assessment. Scale 1-10.\n    - Mistral (40%): Pure humor rating (0-4 scale, normalized to 0-10).\n    \n    GPT self-bias discount: GPT evaluation scores are multiplied by 0.80\n    for GPT-generated jokes to counteract GPT rating its own output higher.\n    \"\"\"\n    jokes = state[\"candidate_jokes\"]\n    word1 = state[\"word1\"]\n    word2 = state[\"word2\"]\n    sources = state.get(\"joke_sources\", [])\n    iteration = state.get(\"iteration_count\", 0)\n\n    print(f\"\\n[Iteration {iteration + 1}] Scoring {len(jokes)} jokes with GPT-OSS + Mistral...\")\n\n    # --- GPT-OSS: multi-criteria quality (1-10) ---\n    print(\"   Scoring with GPT-OSS (batch mode - single API call)...\")\n    gpt_scores_raw = llm_scorer.score_jokes(jokes, word1, word2, verbose=False)\n\n    # Apply GPT self-bias discount for GPT-generated jokes\n    GPT_SELF_BIAS_DISCOUNT = 0.80\n    gpt_scores = []\n    for i, score in enumerate(gpt_scores_raw):\n        src = sources[i] if i < len(sources) else \"?\"\n        if src == \"GPT\":\n            gpt_scores.append(score * GPT_SELF_BIAS_DISCOUNT)\n        else:\n            gpt_scores.append(score)\n\n    # --- Mistral: pure humor rating (0-4) ---\n    print(\"   Scoring with Mistral (humor funniness only)...\")\n    mistral_scores = mistral_scorer.score_jokes(jokes, word1, word2, verbose=False)\n    \n    # Normalize Mistral to 0-10 using distribution-aware mapping\n    mistral_scores_normalized = [normalize_mistral_score(s) for s in mistral_scores]\n    \n    # Weighted ensemble: GPT 60% + Mistral 40%\n    W_GPT = 0.6\n    W_MISTRAL = 0.4\n    \n    final_scores = [\n        W_GPT * gpt + W_MISTRAL * mistral\n        for gpt, mistral in zip(gpt_scores, mistral_scores_normalized)\n    ]\n\n    # Print score breakdown\n    print(f\"\\n   Score breakdown (GPT {W_GPT:.0%} + Mistral {W_MISTRAL:.0%}, GPT self-bias discount={GPT_SELF_BIAS_DISCOUNT}):\")\n    print(f\"   {'='*120}\")\n    print(f\"   {'#':>3} {'Src':<6} {'GPT_raw':>7} {'GPT_adj':>7}  {'Mistral':>7} {'(norm)':>7}  {'Final':>7}  Joke\")\n    print(f\"   {'-'*120}\")\n    for i, (joke, gpt_raw, gpt_adj, m_raw, m_norm, final) in enumerate(\n        zip(jokes, gpt_scores_raw, gpt_scores, mistral_scores, mistral_scores_normalized, final_scores)\n    ):\n        src = sources[i] if i < len(sources) else \"?\"\n        disc = \"*\" if src == \"GPT\" else \" \"\n        print(f\"   {i+1:3d} {src:<6} {gpt_raw:5.2f}/10 {gpt_adj:5.2f}{disc}   {m_raw:5.2f}/4 ({m_norm:5.2f})  {final:5.2f}/10  {joke[:60]}...\")\n\n    # Find best joke\n    if final_scores:\n        best_idx = final_scores.index(max(final_scores))\n        best_joke = jokes[best_idx]\n        best_score = final_scores[best_idx]\n    else:\n        best_joke = \"\"\n        best_score = 0.0\n\n    print(f\"\\n   * = GPT score discounted by {GPT_SELF_BIAS_DISCOUNT} (GPT-generated joke)\")\n    print(f\"\\n   Best:  {best_score:.2f}/10 (GPT: {gpt_scores[best_idx]:.2f}, Mistral: {mistral_scores[best_idx]:.2f}/4)\")\n    print(f\"   Avg:   {sum(final_scores)/len(final_scores):.2f}/10\")\n    print(f\"   Range: {min(final_scores):.2f} - {max(final_scores):.2f}\")\n\n    return {\n        **state,\n        \"scores\": final_scores,\n        \"gpt_scores\": gpt_scores,\n        \"mistral_scores\": mistral_scores,\n        \"best_joke\": best_joke,\n        \"best_score\": best_score\n    }\n\n\ndef check_score_threshold(state: JokeGenerationState) -> Literal[\"refine\", \"output\"]:\n    \"\"\"Decision node: check if score meets threshold\"\"\"\n    score = state[\"best_score\"]\n    iteration = state.get(\"iteration_count\", 0)\n    max_iter = state.get(\"max_iterations\", 3)\n\n    print(f\"\\nDecision: Score={score:.2f}, Iteration={iteration + 1}/{max_iter}\")\n\n    if score >= 7.0:\n        print(f\"   Score threshold met! (>=7.0)\")\n        return \"output\"\n    elif iteration >= max_iter - 1:\n        print(f\"   Max iterations reached, outputting best result\")\n        return \"output\"\n    else:\n        print(f\"   Score below threshold, refining...\")\n        return \"refine\"\n\n\ndef increment_iteration(state: JokeGenerationState) -> JokeGenerationState:\n    \"\"\"Increment iteration counter and save history\"\"\"\n    current_iter = state.get(\"iteration_count\", 0)\n    history = state.get(\"history\", [])\n\n    # Save this iteration's results\n    history.append({\n        \"iteration\": current_iter + 1,\n        \"best_score\": state[\"best_score\"],\n        \"best_joke\": state[\"best_joke\"],\n        \"num_candidates\": len(state.get(\"candidate_jokes\", [])),\n        \"avg_score\": sum(state.get(\"scores\", [0])) / max(len(state.get(\"scores\", [1])), 1),\n        \"avg_gpt_score\": sum(state.get(\"gpt_scores\", [0])) / max(len(state.get(\"gpt_scores\", [1])), 1),\n        \"avg_mistral_score\": sum(state.get(\"mistral_scores\", [0])) / max(len(state.get(\"mistral_scores\", [1])), 1)\n    })\n\n    return {\n        **state,\n        \"iteration_count\": current_iter + 1,\n        \"history\": history\n    }\n\n\ndef output_result(state: JokeGenerationState) -> JokeGenerationState:\n    \"\"\"Final output node - shows best and second-best joke with source\"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"FINAL RESULT\")\n    print(f\"{'='*80}\")\n\n    word1 = state['word1']\n    word2 = state['word2']\n    if word2:\n        print(f\"\\nWord Pair: '{word1}' + '{word2}'\")\n    else:\n        print(f\"\\nHeadline: '{word1}'\")\n    print(f\"Total Iterations: {state.get('iteration_count', 0) + 1}\")\n\n    # Rank all jokes by final score\n    jokes = state.get(\"candidate_jokes\", [])\n    scores = state.get(\"scores\", [])\n    gpt_sc = state.get(\"gpt_scores\", [])\n    mis_sc = state.get(\"mistral_scores\", [])\n    sources = state.get(\"joke_sources\", [])\n\n    if jokes and scores:\n        ranked = sorted(zip(jokes, scores, gpt_sc, mis_sc, sources + [\"?\"] * max(0, len(jokes) - len(sources))),\n                        key=lambda x: x[1], reverse=True)\n\n        # Best joke\n        j1, f1, g1, m1, s1 = ranked[0]\n        print(f\"\\n  #1 BEST  [{f1:.2f}/10]  (GPT:{g1:.1f}, Mistral:{m1:.2f}/4)  Source:{s1}\")\n        print(f\"  {j1}\")\n\n        # Second best\n        if len(ranked) >= 2:\n            j2, f2, g2, m2, s2 = ranked[1]\n            print(f\"\\n  #2       [{f2:.2f}/10]  (GPT:{g2:.1f}, Mistral:{m2:.2f}/4)  Source:{s2}\")\n            print(f\"  {j2}\")\n    else:\n        print(f\"\\n  (no jokes scored)\")\n\n    print(f\"\\n{'='*80}\\n\")\n\n    return state\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6704d06",
   "metadata": {},
   "outputs": [],
   "source": "\n\n# ============================================================================\n# GRAPH BUILDER\n# ============================================================================\n\ndef build_joke_generation_graph(\n    llama_generator: RAGHumorGenerator,\n    gpt_generator: GPT4JokeGenerator,\n    llm_scorer: LLMHumorScorer,\n    mistral_scorer: MistralHumorScorer\n):\n    \"\"\"\n    Build the LangGraph for joke generation pipeline with dual evaluation.\n\n    Args:\n        llama_generator: RAGHumorGenerator\n        gpt_generator: GPT4JokeGenerator\n        llm_scorer: LLMHumorScorer\n        mistral_scorer: MistralHumorScorer\n\n    Returns:\n        Compiled LangGraph\n    \"\"\"\n    workflow = StateGraph(JokeGenerationState)\n\n    # Add nodes with bound generators/scorers\n    workflow.add_node(\n        \"start\",\n        lambda state: state  # pass-through to fan out to both generators\n    )\n    workflow.add_node(\n        \"generate_llama\",\n        lambda state: generate_jokes_llama(state, llama_generator)\n    )\n    workflow.add_node(\n        \"generate_gpt\",\n        lambda state: generate_jokes_gpt(state, gpt_generator)\n    )\n    workflow.add_node(\"combine_jokes\", combine_jokes)\n    workflow.add_node(\n        \"score_jokes\",\n        lambda state: score_jokes_dual(state, llm_scorer, mistral_scorer)\n    )\n    workflow.add_node(\"increment_iteration\", increment_iteration)\n    workflow.add_node(\"output\", output_result)\n\n    # Fan-out: start node dispatches to both generators in parallel\n    workflow.set_entry_point(\"start\")\n    workflow.add_edge(\"start\", \"generate_llama\")\n    workflow.add_edge(\"start\", \"generate_gpt\")\n\n    # Add edges - both generators feed into combine\n    workflow.add_edge(\"generate_llama\", \"combine_jokes\")\n    workflow.add_edge(\"generate_gpt\", \"combine_jokes\")\n\n    # After combining, score the jokes\n    workflow.add_edge(\"combine_jokes\", \"score_jokes\")\n\n    # Decision point: refine or output\n    workflow.add_conditional_edges(\n        \"score_jokes\",\n        check_score_threshold,\n        {\n            \"refine\": \"increment_iteration\",\n            \"output\": \"output\"\n        }\n    )\n\n    # Refinement loop - go back to start (fans out to both generators)\n    workflow.add_edge(\"increment_iteration\", \"start\")\n\n    # End the workflow\n    workflow.add_edge(\"output\", END)\n\n    return workflow.compile()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3f9bc",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# LOAD MODELS (run once, then skip when re-running pipeline)\n# ============================================================================\n\nprint(\"=\"*80)\nprint(\"LANGGRAPH JOKE GENERATION PIPELINE - DUAL EVALUATOR MODE\")\nprint(\"GPT-OSS (60%) + Mistral (40%) Ensemble Scoring\")\nprint(\"=\"*80)\nprint()\n\nfrom dotenv import load_dotenv\nload_dotenv(Path(\"../../.env\"))\n\nNVIDIA_API_KEY = os.getenv(\"NVIDIA_API_KEY\")\nif not NVIDIA_API_KEY:\n    raise ValueError(\"NVIDIA_API_KEY not found in environment. Please set it in .env file\")\n\nconfig = RAGConfig()\nconfig.TEMPERATURE = 0.65\nconfig.TOP_K = 50\n\nprint(\"Initializing components...\\n\")\n\n# 1. Llama Generator\ntry:\n    llama_gen = RAGHumorGenerator(config)\n    llama_gen.load_model()\n    print(\"Llama generator initialized\\n\")\nexcept Exception as e:\n    print(f\"Could not load Llama model: {e}\\n\")\n    llama_gen = None\n\n# 2. GPT-OSS Generator\ngpt_gen = GPT4JokeGenerator(\n    api_key=NVIDIA_API_KEY,\n    temperature=0.8,\n    max_retries=1\n)\nprint(\"GPT-OSS generator initialized\\n\")\n\n# 3. GPT-OSS Scorer\nllm_scorer = LLMHumorScorer(\n    api_key=NVIDIA_API_KEY,\n    temperature=0.4,\n    max_retries=2\n)\nprint(\"GPT-OSS scorer initialized\\n\")\n\n# 4. Mistral Scorer (auto-detects v2 checkpoint with recovered head weights)\nmistral_scorer = MistralHumorScorer(\n    base_model_id=\"mistralai/Ministral-3-8B-Base-2512\",\n    num_labels=2,\n    max_length=512\n)\nprint(f\"Mistral checkpoint: {mistral_scorer.checkpoint_path}\")\n\nif not mistral_scorer.checkpoint_path.exists():\n    print(f\"Mistral checkpoint not found! Expected: {mistral_scorer.checkpoint_path}\")\n    mistral_scorer = None\nelse:\n    mistral_scorer.load_model()\n    print(\"Mistral scorer initialized\\n\")\n\nprint(\"=\"*80)\nprint(\"All models loaded.\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7s1hlovy1ex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BUILD RAG DATABASE & COMPILE GRAPH (run once after loading models)\n",
    "# ============================================================================\n",
    "\n",
    "# Load RAG database\n",
    "if llama_gen is not None:\n",
    "    llama_gen.load_rag_database(build_if_missing=True)\n",
    "    print(\"RAG database loaded\\n\")\n",
    "\n",
    "# Build the graph\n",
    "if mistral_scorer is None:\n",
    "    raise RuntimeError(\"Cannot run dual evaluator mode without Mistral scorer\")\n",
    "if llama_gen is None:\n",
    "    raise RuntimeError(\"Cannot run pipeline without Llama generator\")\n",
    "\n",
    "graph = build_joke_generation_graph(llama_gen, gpt_gen, llm_scorer, mistral_scorer)\n",
    "print(\"LangGraph compiled (Dual Evaluator mode)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h76t77amim",
   "metadata": {},
   "outputs": [],
   "source": "\n# ============================================================================\n# RUN PIPELINE (re-run this cell to regenerate jokes)\n# ============================================================================\nimport csv\nfrom datetime import datetime\n\n# Supports two input modes:\n#   (\"word1\", \"word2\")  -> word-pair mode: joke connecting both words\n#   (\"headline text\", \"\") -> headline mode: funny commentary on headline\ntest_pairs = [\n    (\"banana\", \"satellite\"),\n    (\"angry\", \"teacup\"),\n    (\"cosmic\", \"toaster\"),\n    (\"sleepy\", \"volcano\"),\n    (\"neon\", \"hamster\"),\n    (\"awkward\", \"penguin\"),\n    (\"electric\", \"spaghetti\"),\n    (\"dramatic\", \"cactus\"),\n    (\"confused\", \"astronaut\"),\n    (\"rubber\", \"tornado\"),\n    (\"invisible\", \"taco\"),\n    (\"sarcastic\", \"dolphin\"),\n    (\"haunted\", \"calculator\"),\n    (\"spicy\", \"cloud\"),\n    (\"polite\", \"chainsaw\"),\n    (\"disco\", \"librarian\"),\n    (\"lonely\", \"robot\"),\n    (\"caffeinated\", \"squirrel\"),\n    (\"confused\", \"mailbox\"),\n    # Headline mode examples (word2 = \"\"):\n    # (\"Scientists discover that cats have been faking sleep for centuries\", \"\"),\n    # (\"Local man wins lottery, immediately buys 10000 rubber ducks\", \"\"),\n]\n\nresults_summary = []\n\n# --- Open CSVs BEFORE the loop so results are saved incrementally ---\noutput_dir = PROJECT_ROOT / \"output\"\noutput_dir.mkdir(exist_ok=True)\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\nbest_csv_path = output_dir / f\"best_jokes_{timestamp}.csv\"\nall_csv_path  = output_dir / f\"all_jokes_{timestamp}.csv\"\n\nbest_csv_file = open(best_csv_path, 'w', newline='', encoding='utf-8')\nall_csv_file  = open(all_csv_path,  'w', newline='', encoding='utf-8')\n\nbest_writer = csv.writer(best_csv_file)\nall_writer  = csv.writer(all_csv_file)\n\nbest_writer.writerow([\"#\", \"Word1\", \"Word2\", \"Best_Joke\", \"Best_Source\", \"2nd_Best_Joke\", \"2nd_Source\",\n                       \"Best_GPT\", \"Best_Mistral_0to4\", \"Best_Final\",\n                       \"2nd_GPT\",  \"2nd_Mistral_0to4\",  \"2nd_Final\", \"Human_Rating\"])\nall_writer.writerow([\"#\", \"PairIdx\", \"Rank\", \"Word1\", \"Word2\", \"Joke\", \"Source\",\n                      \"GPT_Score\", \"Mistral_0to4\", \"Mistral_Norm\", \"Final_0to10\", \"Human_Rating\"])\n\nall_row_num = 1\nprint(f\"Incremental CSVs: {best_csv_path.name} / {all_csv_path.name}\")\n\nfor pair_idx, (word1, word2) in enumerate(test_pairs, 1):\n    is_headline = not word2\n    if is_headline:\n        print(\"\\n\" + \"=\"*80)\n        print(f\"PROCESSING [{pair_idx}/{len(test_pairs)}] HEADLINE: '{word1}'\")\n        print(\"=\"*80)\n    else:\n        print(\"\\n\" + \"=\"*80)\n        print(f\"PROCESSING [{pair_idx}/{len(test_pairs)}]: '{word1}' + '{word2}'\")\n        print(\"=\"*80)\n\n    initial_state = {\n        \"word1\": word1, \"word2\": word2,\n        \"llama_jokes\": [], \"gpt_jokes\": [], \"candidate_jokes\": [],\n        \"joke_sources\": [],\n        \"scores\": [], \"gpt_scores\": [], \"mistral_scores\": [],\n        \"best_joke\": \"\", \"best_score\": 0.0,\n        \"iteration_count\": 0, \"max_iterations\": 1, \"history\": []\n    }\n\n    try:\n        result = graph.invoke(initial_state)\n        total_candidates = len(result.get('candidate_jokes', []))\n\n        label = f\"Headline: {word1}\" if is_headline else f\"{word1} + {word2}\"\n        r = {\n            \"word_pair\": label,\n            \"word1\": word1,\n            \"word2\": word2,\n            \"best_score\": result['best_score'],\n            \"best_joke\": result['best_joke'],\n            \"all_jokes\": result.get('candidate_jokes', []),\n            \"all_scores\": result.get('scores', []),\n            \"gpt_scores\": result.get('gpt_scores', []),\n            \"mistral_scores\": result.get('mistral_scores', []),\n            \"joke_sources\": result.get('joke_sources', []),\n            \"iterations\": result.get('iteration_count', 0) + 1,\n            \"total_candidates\": total_candidates\n        }\n        results_summary.append(r)\n\n        # --- Write to CSVs immediately after each word pair ---\n        sources = r['joke_sources']\n        jd = list(zip(r['all_jokes'], r['all_scores'], r['gpt_scores'], r['mistral_scores'],\n                       sources + [\"?\"] * max(0, len(r['all_jokes']) - len(sources))))\n        jd.sort(key=lambda x: x[1], reverse=True)\n\n        # Best + 2nd best row\n        if len(jd) >= 2:\n            j1, f1, g1, m1, s1 = jd[0]\n            j2, f2, g2, m2, s2 = jd[1]\n        elif len(jd) == 1:\n            j1, f1, g1, m1, s1 = jd[0]\n            j2, f2, g2, m2, s2 = \"\", 0, 0, 0, \"\"\n        else:\n            j1, f1, g1, m1, s1 = \"\", 0, 0, 0, \"\"\n            j2, f2, g2, m2, s2 = \"\", 0, 0, 0, \"\"\n\n        best_writer.writerow([\n            pair_idx, word1, word2,\n            j1, s1, j2, s2,\n            f\"{g1:.2f}\", f\"{m1:.2f}\", f\"{f1:.2f}\",\n            f\"{g2:.2f}\", f\"{m2:.2f}\", f\"{f2:.2f}\", \"\"\n        ])\n        best_csv_file.flush()\n\n        # All jokes for this pair\n        for rank, (joke, final, gpt, mistral, src) in enumerate(jd, 1):\n            m_norm = normalize_mistral_score(mistral)\n            all_writer.writerow([\n                all_row_num, pair_idx, rank, word1, word2, joke, src,\n                f\"{gpt:.2f}\", f\"{mistral:.2f}\", f\"{m_norm:.2f}\", f\"{final:.2f}\", \"\"\n            ])\n            all_row_num += 1\n        all_csv_file.flush()\n\n    except Exception as e:\n        print(f\"\\nError processing word pair: {e}\")\n        import traceback\n        traceback.print_exc()\n\nbest_csv_file.close()\nall_csv_file.close()\n\nprint(f\"\\nDone! Generated jokes for {len(results_summary)}/{len(test_pairs)} entries.\")\nprint(f\"Best+2nd CSV : {best_csv_path}\")\nprint(f\"All jokes CSV: {all_csv_path}  ({all_row_num - 1} jokes)\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t80fj6797u",
   "metadata": {},
   "outputs": [],
   "source": "\n# ============================================================================\n# FULL OUTPUT FOR HUMAN RATING (re-run to reformat without regenerating)\n# ============================================================================\n\nprint(\"#\"*80)\nprint(\"#\" + \" \"*26 + \"JOKES FOR HUMAN RATING\" + \" \"*30 + \"#\")\nprint(\"#\"*80)\nprint(f\"\\nTotal entries: {len(results_summary)}\")\nprint(f\"Total jokes generated: {sum(r['total_candidates'] for r in results_summary)}\")\nprint(f\"Scoring: GPT-OSS 60% (multi-criteria) + Mistral 40% (humor only)\")\nprint(f\"GPT self-bias discount: 0.80 for GPT-generated jokes\")\nprint()\n\nfor pair_idx, r in enumerate(results_summary, 1):\n    print(\"=\"*80)\n    if r['word2']:\n        print(f\"WORD PAIR {pair_idx}/{len(results_summary)}: '{r['word1']}' + '{r['word2']}'\")\n    else:\n        print(f\"HEADLINE {pair_idx}/{len(results_summary)}: '{r['word1']}'\")\n    print(f\"Best Score: {r['best_score']:.2f}/10\")\n    print(\"=\"*80)\n\n    # Sort jokes by final score (best first)\n    sources = r.get('joke_sources', [])\n    joke_data = list(zip(\n        r['all_jokes'], r['all_scores'], r['gpt_scores'], r['mistral_scores'],\n        sources + [\"?\"] * max(0, len(r['all_jokes']) - len(sources))\n    ))\n    joke_data.sort(key=lambda x: x[1], reverse=True)\n\n    for j_idx, (joke, final, gpt, mistral, src) in enumerate(joke_data, 1):\n        mistral_norm = normalize_mistral_score(mistral)\n        is_best = \" << SELECTED\" if joke == r['best_joke'] else \"\"\n        print(f\"\\n  [{j_idx:2d}] Score: {final:.2f}/10  (GPT: {gpt:.1f}, Mistral: {mistral:.2f}/4 -> {mistral_norm:.1f}/10)  Src:{src}{is_best}\")\n        print(f\"  {'-'*70}\")\n        print(f\"  {joke}\")\n\n    print()\n\n# --- Summary table ---\nprint(\"=\"*80)\nprint(\"RESULTS SUMMARY\")\nprint(\"=\"*80)\nprint(f\"\\n{'#':<4} {'Input':<35} {'Score':<8} {'GPT':<8} {'Mistral':<8} {'BestSrc':<8} {'Jokes':<8}\")\nprint(\"-\"*85)\nfor i, r in enumerate(results_summary, 1):\n    avg_gpt = sum(r['gpt_scores']) / len(r['gpt_scores']) if r['gpt_scores'] else 0\n    avg_mis = sum(r['mistral_scores']) / len(r['mistral_scores']) if r['mistral_scores'] else 0\n    label = r['word_pair'][:33]\n    # Find source of best joke\n    sources = r.get('joke_sources', [])\n    jokes = r['all_jokes']\n    scores = r['all_scores']\n    best_src = \"?\"\n    if jokes and scores:\n        best_idx = scores.index(max(scores))\n        best_src = sources[best_idx] if best_idx < len(sources) else \"?\"\n    print(f\"{i:<4} {label:<35} {r['best_score']:<8.2f} {avg_gpt:<8.1f} {avg_mis:<8.2f} {best_src:<8} {r['total_candidates']:<8}\")\n\navg_score = sum(r['best_score'] for r in results_summary) / len(results_summary) if results_summary else 0\nprint(\"-\"*85)\nprint(f\"{'':4} {'AVERAGE':<35} {avg_score:<8.2f}\")\n\n# --- Save JSON (full data for programmatic access) ---\noutput_dir = PROJECT_ROOT / \"output\"\noutput_dir.mkdir(exist_ok=True)\njson_path = output_dir / f\"jokes_for_rating_{timestamp}.json\"\nwith open(json_path, 'w', encoding='utf-8') as f:\n    json.dump(results_summary, f, indent=2, ensure_ascii=False)\nprint(f\"\\nJSON saved: {json_path}\")\nprint(f\"CSVs already saved incrementally during pipeline run:\")\nprint(f\"  Best+2nd : {best_csv_path}\")\nprint(f\"  All jokes: {all_csv_path}\")\nprint(f\"\\n  -> Open CSVs in Excel and fill 'Human_Rating' column (1-10)\")\nprint(f\"\\n{'='*80}\")\nprint(\"Pipeline complete!\")\nprint(\"=\"*80)\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}